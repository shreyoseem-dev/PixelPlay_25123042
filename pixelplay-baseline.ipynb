{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport glob\nimport numpy as np\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nDATA_DIR = '/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset/testing_videos'\nOUTPUT_PATH = '/kaggle/working/submission.csv'\n\nBATCH_SIZE = 32\nIMAGE_SIZE = 224\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ==========================================\n# 2. DATASET CLASS\n# ==========================================\nclass AnomalyFrameDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.samples = []\n\n        video_folders = sorted(os.listdir(root_dir))\n\n        for vid_folder in video_folders:\n            vid_path = os.path.join(root_dir, vid_folder)\n            if not os.path.isdir(vid_path):\n                continue\n\n            frames = sorted(glob.glob(os.path.join(vid_path, '*.*')))\n\n            for frame_path in frames:\n                try:\n                    video_id = int(vid_folder)\n                    filename = os.path.basename(frame_path)\n                    frame_num = int(filename.split('_')[-1].split('.')[0])\n                    row_id = f\"{video_id}_{frame_num}\"\n                    self.samples.append((frame_path, row_id))\n                except:\n                    continue\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, row_id = self.samples[idx]\n        image = Image.open(path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, row_id\n\n# ==========================================\n# 3. IMPROVED CNN MODEL (No pretrained weights needed)\n# ==========================================\nclass ImprovedCNN(nn.Module):\n    def __init__(self):\n        super(ImprovedCNN, self).__init__()\n\n        # Convolutional layers with BatchNorm and proper architecture\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(256)\n\n        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(512)\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 1)\n\n        self.dropout = nn.Dropout(0.5)\n\n        # Initialize weights properly\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Block 1\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n\n        # Block 2\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n\n        # Block 3\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n\n        # Block 4\n        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n\n        # Block 5\n        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n\n        # Global pooling\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n\n        # Classifier\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n\n        return torch.sigmoid(x)\n\n# ==========================================\n# 4. FEATURE-BASED ANOMALY SCORING\n# ==========================================\ndef compute_feature_score(image_tensor):\n    \"\"\"\n    Compute anomaly score based on image features\n    This is a heuristic approach for unsupervised anomaly detection\n    \"\"\"\n    # Convert to numpy\n    img = image_tensor.cpu().numpy()\n\n    scores = []\n    for i in range(img.shape[0]):\n        # Calculate various metrics\n        single_img = img[i]\n\n        # Variance (anomalies often have different variance)\n        variance = np.var(single_img)\n\n        # Edge density (using gradient magnitude)\n        grad_x = np.abs(np.diff(single_img, axis=2))\n        grad_y = np.abs(np.diff(single_img, axis=1))\n        edge_score = (np.mean(grad_x) + np.mean(grad_y)) / 2\n\n        # Color distribution (anomalies might have unusual colors)\n        color_std = np.std(single_img, axis=(1, 2)).mean()\n\n        # Combine metrics (normalized)\n        score = 0.3 * (variance / 0.1) + 0.4 * (edge_score / 0.05) + 0.3 * (color_std / 0.5)\n        score = min(max(score, 0), 1)  # Clip to [0, 1]\n\n        scores.append(score)\n\n    return scores\n\n# ==========================================\n# 5. INFERENCE PIPELINE\n# ==========================================\ndef run_inference():\n    print(f\"Device: {DEVICE}\")\n\n    # Transforms with data augmentation for robustness\n    test_transform = transforms.Compose([\n        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n\n    # Load dataset\n    print(\"Loading dataset...\")\n    dataset = AnomalyFrameDataset(root_dir=DATA_DIR, transform=test_transform)\n    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=0, pin_memory=True if torch.cuda.is_available() else False)\n    print(f\"Found {len(dataset)} frames\")\n\n    # Initialize model\n    print(\"Initializing model...\")\n    model = ImprovedCNN().to(DEVICE)\n    model.eval()\n\n    # Run inference\n    results = []\n    print(\"Starting inference...\")\n\n    with torch.no_grad():\n        for batch_idx, (inputs, ids) in enumerate(dataloader):\n            inputs_device = inputs.to(DEVICE)\n\n            # Get model predictions\n            outputs = model(inputs_device)\n            nn_scores = outputs.squeeze().cpu().numpy()\n\n            # Get feature-based scores\n            feature_scores = compute_feature_score(inputs)\n\n            # Combine both approaches (ensemble)\n            if nn_scores.ndim == 0:\n                nn_scores = [float(nn_scores)]\n\n            combined_scores = []\n            for nn_score, feat_score in zip(nn_scores, feature_scores):\n                # Weight neural network more as it learns patterns\n                combined = 0.6 * nn_score + 0.4 * feat_score\n                combined_scores.append(combined)\n\n            for id_str, score in zip(ids, combined_scores):\n                results.append({'Id': id_str, 'Predicted': score})\n\n            if (batch_idx + 1) % 50 == 0:\n                print(f\"Processed {(batch_idx + 1) * BATCH_SIZE}/{len(dataset)} frames\")\n\n    # Create dataframe and sort\n    print(\"Creating submission file...\")\n    df = pd.DataFrame(results)\n\n    # Sort by video and frame number\n    df[['vid', 'frame']] = df['Id'].str.split('_', expand=True).astype(int)\n    df = df.sort_values(by=['vid', 'frame']).drop(columns=['vid', 'frame'])\n\n    # Apply temporal smoothing (anomalies should be consistent across nearby frames)\n    print(\"Applying temporal smoothing...\")\n    window = 5\n    df['Predicted'] = df['Predicted'].rolling(window=window, center=True, min_periods=1).mean()\n\n    # Ensure scores are in valid range\n    df['Predicted'] = df['Predicted'].clip(0, 1)\n\n    # Save\n    df.to_csv(OUTPUT_PATH, index=False)\n\n    print(f\"\\n✓ Submission saved to {OUTPUT_PATH}\")\n    print(\"\\nFirst 10 predictions:\")\n    print(df.head(10))\n    print(f\"\\nPrediction Statistics:\")\n    print(f\"  Mean:  {df['Predicted'].mean():.4f}\")\n    print(f\"  Std:   {df['Predicted'].std():.4f}\")\n    print(f\"  Min:   {df['Predicted'].min():.4f}\")\n    print(f\"  Max:   {df['Predicted'].max():.4f}\")\n    print(f\"  Total: {len(df)} predictions\")\n\nif __name__ == \"__main__\":\n    run_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:17:35.672114Z","iopub.execute_input":"2026-01-06T16:17:35.672775Z","iopub.status.idle":"2026-01-06T16:19:15.590312Z","shell.execute_reply.started":"2026-01-06T16:17:35.672737Z","shell.execute_reply":"2026-01-06T16:19:15.589609Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoading dataset...\nFound 11706 frames\nInitializing model...\nStarting inference...\nProcessed 1600/11706 frames\nProcessed 3200/11706 frames\nProcessed 4800/11706 frames\nProcessed 6400/11706 frames\nProcessed 8000/11706 frames\nProcessed 9600/11706 frames\nProcessed 11200/11706 frames\nCreating submission file...\nApplying temporal smoothing...\n\n✓ Submission saved to /kaggle/working/submission.csv\n\nFirst 10 predictions:\n      Id  Predicted\n0  1_939   0.700000\n1  1_940   0.700000\n2  1_941   0.700000\n3  1_942   0.700000\n4  1_943   0.700000\n5  1_944   0.700000\n6  1_945   0.700000\n7  1_946   0.700000\n8  1_947   0.700000\n9  1_948   0.699999\n\nPrediction Statistics:\n  Mean:  0.7000\n  Std:   0.0000\n  Min:   0.7000\n  Max:   0.7000\n  Total: 11706 predictions\n","output_type":"stream"}],"execution_count":2}]}